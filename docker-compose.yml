services:
  dots-ocr:
    image: ${VLLM_IMAGE:-vllm/vllm-openai:v0.11.0}

    container_name: dots-ocr-service

    ports:
      - "${API_PORT:-8000}:8000"

    # Supporto GPU NVIDIA
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    volumes:
      # Cache HuggingFace persistente
      - huggingface-cache:/root/.cache/huggingface
      # Montaggio facoltativo per persistere i logs
      - ocr-logs:/var/log/vllm

    environment:
      # Disabilita cache locale per forza download HuggingFace
      - HF_HUB_OFFLINE=0
      # Livello logging di vLLM
      - VLLM_LOGGING_LEVEL=INFO
      # Timeout per connessioni
      - VLLM_HTTP_TIMEOUT=300
      # Token di autenticazione (opzionale)
      - VLLM_TOKEN=${VLLM_TOKEN:-your-secret-token-here}
      # Token HuggingFace per download modelli (opzionale)
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      # Memoria CUDA: evita frammentazione
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

    # Comando vLLM per dots.ocr
    command: >
      --model ${OCR_MODEL:-rednote-hilab/dots.ocr}
      --dtype auto
      --quantization ${QUANTIZATION:-bitsandbytes}
      --load-format ${LOAD_FORMAT:-bitsandbytes}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.90}
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --max-num-seqs ${MAX_NUM_SEQS:-4}
      --max-num-batched-tokens 4096
      --enforce-eager
      --trust-remote-code
      --served-model-name dots-ocr
      --chat-template-content-format string
      --disable-log-requests
      --limit-mm-per-prompt '{"image": ${MAX_IMAGES_PER_PROMPT:-6}}'

    # Healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

    restart: unless-stopped

    # Shared memory aumentata per workload GPU
    shm_size: '8gb'

  # Versione CPU-only (avviare con: docker compose --profile cpu up -d)
  dots-ocr-cpu:
    image: ${VLLM_CPU_IMAGE:-public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:latest}

    container_name: dots-ocr-cpu-service
    profiles:
      - cpu

    ports:
      - "${API_PORT:-8000}:8000"

    volumes:
      - huggingface-cache:/root/.cache/huggingface
      - ocr-logs:/var/log/vllm

    environment:
      - HF_HUB_OFFLINE=0
      - VLLM_LOGGING_LEVEL=INFO
      - VLLM_HTTP_TIMEOUT=600
      - VLLM_TOKEN=${VLLM_TOKEN:-your-secret-token-here}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - VLLM_TARGET_DEVICE=cpu

    command: >
      --model ${OCR_MODEL:-rednote-hilab/dots.ocr}
      --dtype float32
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --max-num-seqs 1
      --trust-remote-code
      --served-model-name dots-ocr
      --chat-template-content-format string
      --disable-log-requests
      --limit-mm-per-prompt '{"image": ${MAX_IMAGES_PER_PROMPT:-6}}'

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s

    restart: unless-stopped
    shm_size: '8gb'

volumes:
  huggingface-cache:
    driver: local
  ocr-logs:
    driver: local
