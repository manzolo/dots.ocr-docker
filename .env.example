# =============================================================================
# dots.ocr - Environment Configuration
# =============================================================================
#
# Quick setup: copy the example that matches your hardware:
#
#   cp examples/env.gpu-24gb .env   # GPU with >= 24GB VRAM (best quality)
#   cp examples/env.gpu-12gb .env   # GPU with 12GB VRAM (bitsandbytes quantization)
#   cp examples/env.cpu .env        # CPU-only (slow, no GPU required)
#
# Then customize the values below as needed.
# =============================================================================

# === Docker Image ===
# GPU image (vLLM with CUDA support)
VLLM_IMAGE=vllm/vllm-openai:v0.11.0
# CPU image (uncomment to override for CPU-only mode)
# VLLM_CPU_IMAGE=public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:latest

# === OCR Model ===
# Standard model (~3.5GB, requires ~10GB VRAM without quantization)
OCR_MODEL=rednote-hilab/dots.ocr
# AWQ 4-bit quantized model (~0.8GB, for GPUs with limited VRAM)
# OCR_MODEL=sugam24/dots-ocr-awq-4bit

# === Network ===
API_PORT=8000

# === Authentication ===
# Token to protect the vLLM API endpoint
VLLM_TOKEN=your-secret-token-here
# HuggingFace token (only needed for gated models)
# HUGGING_FACE_HUB_TOKEN=hf_your_token_here

# === GPU Settings ===
# VRAM percentage to use (0.0-1.0). Lower if using a desktop GPU
GPU_MEMORY_UTILIZATION=0.90
# Max model context length (lower to save VRAM)
MAX_MODEL_LEN=8192

# === Performance ===
MAX_NUM_SEQS=4
MAX_NUM_BATCHED_TOKENS=8192
MAX_IMAGES_PER_PROMPT=6

# === Extra vLLM Arguments ===
# Append additional flags to the vLLM command (e.g., quantization)
# VLLM_EXTRA_ARGS=--quantization bitsandbytes --load-format bitsandbytes --enforce-eager
VLLM_EXTRA_ARGS=
